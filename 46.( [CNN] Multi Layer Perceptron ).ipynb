{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11d2a91-f617-4ace-b45e-dc55a1ebcd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2491078d-084e-4ec6-9a42-0c087ca76603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4f1ed90-5ce8-4f25-befb-b0f6e8c5efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "373f96ee-e4ea-4be1-b15f-0f627a699dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\My\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn.add(Conv2D(32,(3,3),input_shape = (64,64,3),activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(Conv2D(16,(3,3),activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "652f0b11-72d7-411a-917e-69c201e272cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(64,activation='relu'))\n",
    "cnn.add(Dense(32,activation='relu'))\n",
    "cnn.add(Dense(16,activation='relu'))\n",
    "cnn.add(Dense(8,activation='relu'))\n",
    "cnn.add(Dense(4,activation='relu'))\n",
    "cnn.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b16fcc1-64ac-409f-8e7c-36a800cc61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss='binary_crossentropy',optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59cf794f-e164-4731-8dc8-7114df054ddc",
   "metadata": {},
   "source": [
    "this below code is come from this link =((( https://faroit.com/keras-docs/2.0.2/preprocessing/image/ ))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b2a25e18-5861-46a3-a02f-98e330d717fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 images belonging to 2 classes.\n",
      "Found 26 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - loss: 0.6246 - val_loss: 0.7394\n",
      "Epoch 2/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - loss: 0.6257 - val_loss: 0.7640\n",
      "Epoch 3/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 0.6140 - val_loss: 0.8291\n",
      "Epoch 4/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - loss: 0.6465 - val_loss: 0.8119\n",
      "Epoch 5/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - loss: 0.6410 - val_loss: 0.7647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17f43e97a10>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        r'D:\\train_data',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        r'D:\\test_data',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=4,  # 32//8 = 4 steps\n",
    "    epochs=5,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=3  # 26//8 ≈ 3 steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3711e9-56b9-470c-9bbc-756a582dfbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3cc4bf5d-6844-4582-8692-d9c77dcafbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f23f97f-385d-46be-8772-41ff548e7643",
   "metadata": {},
   "source": [
    "### Input Dog image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "258170b7-c0a3-4dd3-ae5e-8d28b93e6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(r\"D:\\download.jpg\",target_size=(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8e92c829-6f83-4907-9ba6-fbf5b5df93ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[231., 231., 223.],\n",
       "        [229., 228., 223.],\n",
       "        [238., 237., 233.],\n",
       "        ...,\n",
       "        [178., 169., 160.],\n",
       "        [175., 166., 157.],\n",
       "        [171., 162., 153.]],\n",
       "\n",
       "       [[227., 227., 219.],\n",
       "        [226., 225., 220.],\n",
       "        [231., 230., 226.],\n",
       "        ...,\n",
       "        [171., 162., 153.],\n",
       "        [167., 158., 149.],\n",
       "        [161., 152., 143.]],\n",
       "\n",
       "       [[225., 225., 217.],\n",
       "        [223., 222., 217.],\n",
       "        [226., 225., 221.],\n",
       "        ...,\n",
       "        [158., 149., 140.],\n",
       "        [155., 146., 137.],\n",
       "        [153., 144., 135.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[214., 208., 194.],\n",
       "        [215., 207., 194.],\n",
       "        [215., 208., 192.],\n",
       "        ...,\n",
       "        [192., 166., 149.],\n",
       "        [197., 171., 154.],\n",
       "        [196., 178., 154.]],\n",
       "\n",
       "       [[210., 208., 196.],\n",
       "        [213., 209., 197.],\n",
       "        [216., 208., 195.],\n",
       "        ...,\n",
       "        [205., 188., 170.],\n",
       "        [189., 172., 154.],\n",
       "        [195., 178., 158.]],\n",
       "\n",
       "       [[210., 208., 196.],\n",
       "        [213., 209., 197.],\n",
       "        [216., 208., 195.],\n",
       "        ...,\n",
       "        [199., 182., 164.],\n",
       "        [206., 189., 171.],\n",
       "        [198., 181., 161.]]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = image.img_to_array(img)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9cbf50cd-3175-4a54-aea7-053e54045f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.expand_dims(img,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ae60b-4024-44fc-a515-290408bb3f7d",
   "metadata": {},
   "source": [
    "### Predict Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "18ad9208-9428-4870-bc25-8a2e697e1629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2c275d3e-f2b3-4101-a523-345dda665d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n"
     ]
    }
   ],
   "source": [
    "p = cnn.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "906d262e-258c-4680-bfaf-1a8150800786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat\n"
     ]
    }
   ],
   "source": [
    "if p[0][0] < 0.5 :\n",
    "    print(\"Dog\")\n",
    "else : \n",
    "    print(\"Cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8174b5-38f7-4626-a08a-6946616fd98c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
